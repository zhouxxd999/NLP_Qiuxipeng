{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>A series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PhraseId  SentenceId                                             Phrase  \\\n",
       "0         1           1  A series of escapades demonstrating the adage ...   \n",
       "1         2           1  A series of escapades demonstrating the adage ...   \n",
       "2         3           1                                           A series   \n",
       "3         4           1                                                  A   \n",
       "4         5           1                                             series   \n",
       "\n",
       "   Sentiment  \n",
       "0          1  \n",
       "1          2  \n",
       "2          2  \n",
       "3          2  \n",
       "4          2  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/train.tsv/train.tsv',sep='\\t')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A series of escapades demonstrating the adage that what is good for the goose is also good for the gander , some of which occasionally amuses but none of which amounts to much of a story .'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Phrase'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "find file path   : corpora/stopwords.zip/stopwords/\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>series escapades demonstrating adage good goos...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>series escapades demonstrating adage good goose</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PhraseId  SentenceId                                             Phrase  \\\n",
       "0         1           1  series escapades demonstrating adage good goos...   \n",
       "1         2           1    series escapades demonstrating adage good goose   \n",
       "2         3           1                                             series   \n",
       "3         4           1                                                      \n",
       "4         5           1                                             series   \n",
       "\n",
       "   Sentiment  \n",
       "0          1  \n",
       "1          2  \n",
       "2          2  \n",
       "3          2  \n",
       "4          2  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string \n",
    "\n",
    "'''\n",
    "    单词预处理，将单词全部小写，并且去除标点符号\n",
    "'''\n",
    "def preprocessing(phrase):\n",
    "    stop_words = stopwords.words(\"english\")\n",
    "    lower = [phras.lower() for phras in phrase]    # 将字母全部小写\n",
    "    no_punct = [text.translate(str.maketrans('','',string.punctuation)) for text in lower]   # 去掉标点符号\n",
    "    sp = [text.split() for text in no_punct]\n",
    "    nos = []\n",
    "    for sl in sp:\n",
    "        w_list = [w for w in sl if w not in stop_words]\n",
    "        nos.append(w_list)\n",
    "    res = [' '.join(lis) for lis in nos]\n",
    "\n",
    "    return res\n",
    "\n",
    "df['Phrase'] = preprocessing(df['Phrase'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'series escapades demonstrating adage good goose also good gander occasionally amuses none amounts much story'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Phrase'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "频次最高的50个单词:\n",
      "[('film', 6689), ('movie', 5905), ('nt', 3970), ('one', 3609), ('like', 3071), ('story', 2520), ('rrb', 2438), ('lrb', 2098), ('good', 2043), ('characters', 1882), ('much', 1862), ('time', 1747), ('comedy', 1721), ('even', 1597), ('little', 1575), ('funny', 1522), ('way', 1511), ('life', 1484), ('make', 1396), ('movies', 1345), ('love', 1296), ('new', 1278), ('enough', 1248), ('work', 1243), ('us', 1234), ('bad', 1211), ('', 1154), ('something', 1152), ('would', 1118), ('never', 1114), ('director', 1099), ('many', 1094), ('people', 1073), ('made', 1060), ('best', 1059), ('two', 1032), ('makes', 1019), ('action', 1005), ('may', 986), ('plot', 979), ('films', 979), ('could', 969), ('character', 968), ('see', 957), ('well', 944), ('world', 917), ('better', 913), ('audience', 912), ('drama', 894), ('look', 888)]\n"
     ]
    }
   ],
   "source": [
    "phrase = df['Phrase'].values.tolist()\n",
    "sentiment = df['Sentiment'].values.tolist()\n",
    "\n",
    "dic = {}\n",
    "for sentence in phrase:\n",
    "    for word in sentence.split(' '):\n",
    "        dic[word] = dic.get(word,0) + 1\n",
    "dic_sorted_list = sorted(dic,key=lambda x:dic[x],reverse=True)\n",
    "print('频次最高的50个单词:')\n",
    "word_fre = [(dic_sorted_list[i],dic[dic_sorted_list[i]]) for i in range(50)]\n",
    "print(word_fre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search word :    \" spirit \"\n",
      "=========================\n",
      "result      raw    5  class : \n",
      "0 \t 4\n",
      "1 \t 24\n",
      "2 \t 92\n",
      "3 \t 146\n",
      "4 \t 40\n",
      "=========================\n",
      "result transform to 3 class : \n",
      "0 \t 28\n",
      "1 \t 92\n",
      "2 \t 186\n"
     ]
    }
   ],
   "source": [
    "# 查看某一个单词在训练集中各个分类所占的数量\n",
    "\n",
    "phrase = df['Phrase'].values.tolist()\n",
    "sentiment = df['Sentiment'].values.tolist()\n",
    "\n",
    "search_word = 'spirit'\n",
    "\n",
    "result = [0,0,0,0,0]\n",
    "for i in range(len(phrase)):\n",
    "    if search_word in phrase[i].split(' '):\n",
    "        result[sentiment[i]] += 1\n",
    "\n",
    "\n",
    "print('search word :    \"',search_word,'\"')\n",
    "print('=========================')\n",
    "print('result      raw    5  class : ')\n",
    "for i in range(5):\n",
    "    print(i,'\\t',result[i])\n",
    "print('=========================')\n",
    "print('result transform to 3 class : ')\n",
    "result_3 = [result[0]+result[1],result[2],result[3]+result[4]]\n",
    "for i in range(3):\n",
    "    print(i,'\\t',result_3[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:07<00:00, 14.91it/s]\n"
     ]
    }
   ],
   "source": [
    "# 查找所有单词(或者前1000个单词)中，情感分明的单词，即在训练集中negative和positive数量差距明显的单词\n",
    "\n",
    "# 0 -- negative word        2 -- neutral word        4 -- positive word\n",
    "def search_dic_word(phrase,sentiment,search_word):\n",
    "    res = [0,0,0,0,0]\n",
    "    for i in range(len(phrase)):\n",
    "        if search_word in phrase[i].split(' '):\n",
    "            res[sentiment[i]] += 1\n",
    "        \n",
    "    result = [res[0]+res[1],res[2],res[3]+res[4]]\n",
    "    \n",
    "\n",
    "    if (result[0]+result[2]) > 3*result[1]:\n",
    "        if result[0] > 10*result[2] :\n",
    "            #print(word,res,result)\n",
    "            return 0\n",
    "        elif result[2] > 10*result[0]:\n",
    "            #print(word,res,result)\n",
    "            return 4\n",
    "    return 2\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "significance_word = {'negative':[],'positive':[]}\n",
    "phrase = df['Phrase'].values.tolist()\n",
    "sentiment = df['Sentiment'].values.tolist()\n",
    "\n",
    "for word in tqdm(dic_sorted_list[0:1000]):\n",
    "    flag = search_dic_word(phrase,sentiment,word)\n",
    "\n",
    "    if flag == 0:\n",
    "        significance_word['negative'].append(word)\n",
    "    elif flag == 4:\n",
    "        significance_word['positive'].append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative word num:  19\n",
      "positive word num:  38\n",
      "=======================================================================\n",
      "negative word list : \n",
      "['worst', 'lack', 'flat', 'contrived', 'tired', 'mess', 'pretentious', 'stupid', 'ugly', 'worse', 'loud', 'unfunny', 'waste', 'tedious', 'shallow', 'depressing', 'poorly', 'pointless', 'empty']\n",
      "-----------------------------------------\n",
      "positive word list : \n",
      "['moving', 'fascinating', 'enjoyable', 'beautiful', 'powerful', 'charming', 'rich', 'solid', 'rare', 'honest', 'beautifully', 'hilarious', 'touching', 'perfectly', 'remarkable', 'terrific', 'gentle', 'wonderful', 'gorgeous', 'creative', 'nice', 'warm', 'excellent', 'engrossing', 'warmth', 'grace', 'unique', 'impressive', 'inventive', 'colorful', 'sensitive', 'meditation', 'captures', 'sexy', 'witty', 'brings', 'heartfelt', 'playful']\n",
      "\n",
      "***********************************************************************\n",
      "\n",
      "all significant in following list:\n",
      "['worst', 'lack', 'flat', 'contrived', 'tired', 'mess', 'pretentious', 'stupid', 'ugly', 'worse', 'loud', 'unfunny', 'waste', 'tedious', 'shallow', 'depressing', 'poorly', 'pointless', 'empty', 'moving', 'fascinating', 'enjoyable', 'beautiful', 'powerful', 'charming', 'rich', 'solid', 'rare', 'honest', 'beautifully', 'hilarious', 'touching', 'perfectly', 'remarkable', 'terrific', 'gentle', 'wonderful', 'gorgeous', 'creative', 'nice', 'warm', 'excellent', 'engrossing', 'warmth', 'grace', 'unique', 'impressive', 'inventive', 'colorful', 'sensitive', 'meditation', 'captures', 'sexy', 'witty', 'brings', 'heartfelt', 'playful']\n"
     ]
    }
   ],
   "source": [
    "print('negative word num: ',len(significance_word['negative']))\n",
    "print('positive word num: ',len(significance_word['positive']))\n",
    "print('=======================================================================')\n",
    "print('negative word list : ')\n",
    "print(significance_word['negative'])\n",
    "print('-----------------------------------------')\n",
    "print('positive word list : ')\n",
    "print(significance_word['positive'])\n",
    "\n",
    "\n",
    "print('\\n***********************************************************************\\n')\n",
    "print('all significant in following list:')\n",
    "all_significance_word = significance_word['negative']\n",
    "all_significance_word.extend(significance_word['positive'])\n",
    "print(all_significance_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_significance_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\ML_py38_CU\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader,Dataset,TensorDataset\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# 定义数据读取类\n",
    "class MyDataset(Dataset):\n",
    "    \"\"\"\n",
    "    步骤一：继承paddle.io.Dataset类\n",
    "    \"\"\"\n",
    "    def __init__(self,df,significant_word,vocab_path='data/glove.6B/glove.6B.300d.txt',word_len=300):\n",
    "        \"\"\"\n",
    "        步骤二：实现构造函数，定义数据读取方式，划分训练和测试数据集\n",
    "        \"\"\"\n",
    "        super(MyDataset, self).__init__()\n",
    "        \n",
    "        self.vocab_path = vocab_path\n",
    "        self.wordlen = word_len\n",
    "        self.significant_word = significant_word\n",
    "        _, _, self.word_to_vec_map = self.load_glove_embeddings()\n",
    "\n",
    "\n",
    "        self.data =np.nan_to_num(np.array([self.sentence_to_avg(text) for text in df['Phrase']]),nan=0)\n",
    "\n",
    "\n",
    "        self.label = [la for la in df['Sentiment']]\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        步骤三:实现__getitem__方法,定义指定index时如何获取数据,并返回单条数据(训练数据，对应的标签)\n",
    "        \"\"\"\n",
    "        d = torch.tensor(self.data[index],dtype=torch.float32)   \n",
    "        l = torch.tensor(self.label[index],dtype=torch.long)\n",
    "\n",
    "        return d,l\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        步骤四:实现__len__方法:返回数据集总数目\n",
    "        \"\"\"\n",
    "        return self.data.shape[0]\n",
    "    \n",
    "\n",
    "    # 将句子转换为向量\n",
    "    def sentence_to_avg(self,sentence):\n",
    "        words = sentence.lower().strip().split()\n",
    "        \n",
    "        avg = np.zeros(self.wordlen+len(self.significant_word),)\n",
    "        \n",
    "        # 直接返回全零向量\n",
    "        if sentence == '':\n",
    "            return avg\n",
    "\n",
    "        for w in words:\n",
    "            if w in self.word_to_vec_map.keys():  # 如果不在词表里面，则该向量设置为全零\n",
    "                    sign_feat = np.zeros(len(self.significant_word),)\n",
    "                    if w in self.significant_word:\n",
    "                        sign_feat[self.significant_word.index(w)] = 1\n",
    "                    # 将显著词做成一个one-hot向量连接到word2vec向量后\n",
    "                    avg += np.concatenate((self.word_to_vec_map[w],sign_feat))\n",
    "        \n",
    "        avg = avg / len(words)\n",
    "        \n",
    "        return avg\n",
    "\n",
    "    # 加载GloVe词嵌入\n",
    "    def load_glove_embeddings(self):\n",
    "        with open(self.vocab_path, 'r', encoding='utf-8') as f:\n",
    "            words = set()\n",
    "            word_to_vec_map = {}\n",
    "            \n",
    "            for line in f:\n",
    "                line = line.strip().split()\n",
    "                curr_word = line[0]\n",
    "                words.add(curr_word)\n",
    "                word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float32)\n",
    "            \n",
    "            i = 1\n",
    "            words_to_index = {}\n",
    "            index_to_words = {}\n",
    "            for w in sorted(words):\n",
    "                words_to_index[w] = i\n",
    "                index_to_words[i] = w\n",
    "                i = i + 1\n",
    "        return words_to_index, index_to_words, word_to_vec_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df,dev_df = train_test_split(df,test_size=0.1,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MyDataset(train_df,all_significance_word)\n",
    "train_loader = DataLoader(train_dataset,32,shuffle=True)\n",
    "\n",
    "dev_dataset = MyDataset(dev_df,all_significance_word)\n",
    "dev_loader = DataLoader(dev_dataset,32,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练NN模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all class number :  5\n"
     ]
    }
   ],
   "source": [
    "from importlib import import_module\n",
    "\n",
    "model_name = 'NN'\n",
    "\n",
    "x = import_module('models.' + model_name)\n",
    "config = x.Config()\n",
    "print('all class number : ',config.num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4390/4390 [03:19<00:00, 21.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 0.9673 ,train acc:0.596 , dev loss : 0.9913,dev acc : 0.586 \n",
      "saving model ...\n",
      "Epoch [2/50]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4390/4390 [03:19<00:00, 22.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 0.8947 ,train acc:0.630 , dev loss : 0.9437,dev acc : 0.614 \n",
      "saving model ...\n",
      "Epoch [3/50]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4390/4390 [03:25<00:00, 21.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 0.8539 ,train acc:0.644 , dev loss : 0.9354,dev acc : 0.616 \n",
      "saving model ...\n",
      "Epoch [4/50]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4390/4390 [03:26<00:00, 21.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 0.7949 ,train acc:0.679 , dev loss : 0.8974,dev acc : 0.636 \n",
      "saving model ...\n",
      "Epoch [5/50]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4390/4390 [03:29<00:00, 20.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 0.7626 ,train acc:0.686 , dev loss : 0.9057,dev acc : 0.637 \n",
      "Epoch [6/50]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 196/4390 [00:09<03:15, 21.45it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtrain\u001b[39;00m \u001b[39mimport\u001b[39;00m train\n\u001b[0;32m      3\u001b[0m model \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mModel(config)\u001b[39m.\u001b[39mto(config\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m----> 5\u001b[0m train(config,model,train_loader,dev_loader,dev_loader)\n",
      "File \u001b[1;32mc:\\Users\\94969\\Desktop\\Word2Vec_NNClf\\train.py:74\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(config, model, train_iter, dev_iter, test_iter)\u001b[0m\n\u001b[0;32m     71\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m     72\u001b[0m     optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m---> 74\u001b[0m     train_loss_sum \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39;49mcpu()\n\u001b[0;32m     75\u001b[0m     batch_count \u001b[39m=\u001b[39m batch_count \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     76\u001b[0m train_acc,train_loss \u001b[39m=\u001b[39m evaluate_accuracy_gpu(model,train_iter)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from train import train\n",
    "\n",
    "model = x.Model(config).to(config.device)\n",
    "\n",
    "train(config,model,train_loader,dev_loader,dev_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>156061</td>\n",
       "      <td>8545</td>\n",
       "      <td>An intermittently pleasing but mostly routine ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>156062</td>\n",
       "      <td>8545</td>\n",
       "      <td>An intermittently pleasing but mostly routine ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>156063</td>\n",
       "      <td>8545</td>\n",
       "      <td>An</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>156064</td>\n",
       "      <td>8545</td>\n",
       "      <td>intermittently pleasing but mostly routine effort</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>156065</td>\n",
       "      <td>8545</td>\n",
       "      <td>intermittently pleasing but mostly routine</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PhraseId  SentenceId                                             Phrase\n",
       "0    156061        8545  An intermittently pleasing but mostly routine ...\n",
       "1    156062        8545  An intermittently pleasing but mostly routine ...\n",
       "2    156063        8545                                                 An\n",
       "3    156064        8545  intermittently pleasing but mostly routine effort\n",
       "4    156065        8545         intermittently pleasing but mostly routine"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/test.tsv/test.tsv',sep='\\t')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "'''\n",
    "    使用model模型预测test_loader标签\n",
    "'''\n",
    "def predict_test_data(config,model,test_loader):\n",
    "    pred = []\n",
    "    for X,y in tqdm(test_loader):\n",
    "\n",
    "        data = X.to(config.device)\n",
    "        out = model(data)\n",
    "        lab = out.argmax(dim=1) #argmax():返回最大数的索引\n",
    "\n",
    "\n",
    "        pred.extend(lab.detach().cpu().numpy().tolist())\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/test.tsv/test.tsv',sep='\\t')\n",
    "df['Phrase'] = preprocessing(df['Phrase'])\n",
    "\n",
    "df['Sentiment'] = [2] * df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = MyDataset(df,all_significance_word)\n",
    "test_loader = DataLoader(test_dataset,32,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_model name :  output/NN_2023-02-18_16-23-57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2072/2072 [00:06<00:00, 298.88it/s]\n"
     ]
    }
   ],
   "source": [
    "model_name = 'output/NN_2023-02-18_16-23-57'\n",
    "print('load_model name : ',model_name)\n",
    "net = x.Model(config).to(config.device)\n",
    "net.load_state_dict(torch.load(model_name))\n",
    "\n",
    "test_pred = predict_test_data(config,net,test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "Submission_path = 'data/sampleSubmission.csv'\n",
    "submission = pd.read_csv(Submission_path)\n",
    "submission.Sentiment = test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('NN_submission.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_py38_CU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "15dfe2a7027fa260522be6aa9f797f27ab9a7dcdfa5dae664afd56517f0ae045"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
